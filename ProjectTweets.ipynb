{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac2e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16444/1145620337.py:14: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/tmp/ipykernel_16444/1145620337.py:15: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    }
   ],
   "source": [
    "# Importing PySpark related libraries\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import concat_ws, regexp_replace, col, lower, to_date, date_format\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lag, lead, when, coalesce, expr, col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Importing Dash and Plotly for data visualization\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Additional PySpark functions and features\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "# Importing NLTK for natural language processing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Downloading NLTK datasets if needed\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# Importing datetime library\n",
    "from datetime import timedelta, datetime, date\n",
    "######\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf717f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ProjectTweets').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ffa58",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91573341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/user1/ProjectTweets.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61bd395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: integer (nullable = true)\n",
      " |-- 1467810369: long (nullable = true)\n",
      " |-- Mon Apr 06 22:19:45 PDT 2009: string (nullable = true)\n",
      " |-- NO_QUERY: string (nullable = true)\n",
      " |-- _TheSpecialOne_: string (nullable = true)\n",
      " |-- @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5363c840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |1467810672|Mon Apr 06 22:19:49 PDT 2009|NO_QUERY|scotthamilton  |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |\n",
      "|2  |1467810917|Mon Apr 06 22:19:53 PDT 2009|NO_QUERY|mattycus       |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |\n",
      "|3  |1467811184|Mon Apr 06 22:19:57 PDT 2009|NO_QUERY|ElleCTF        |my whole body feels itchy and like its on fire                                                                     |\n",
      "|4  |1467811193|Mon Apr 06 22:19:57 PDT 2009|NO_QUERY|Karoli         |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.     |\n",
      "|5  |1467811372|Mon Apr 06 22:20:00 PDT 2009|NO_QUERY|joy_wolf       |@Kwesidei not the whole crew                                                                                       |\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c358289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = ['ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "for i, column_name in enumerate(new_cols):\n",
    "    df = df.withColumnRenamed(df.columns[i + 1], column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb99dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f77ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_column = df.select('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2baf2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('date', to_date(df['date'], 'EEE MMM dd HH:mm:ss zzz yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a856b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('date', to_date(col('date'), 'dd/MM/yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "572dff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to lowercase and clean unnecessary characters\n",
    "df = df.withColumn(\"text\", lower(regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\\\s]\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc3289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special symbols, and links from text data\n",
    "df = df.withColumn(\"text\", regexp_replace(col(\"text\"), r'[@#]\\w+|https?://\\S+|\\W', \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e34447de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|0  |ids       |date      |flag    |user         |text                                                                                                           |\n",
      "+---+----------+----------+--------+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|1  |1467810672|2009-04-07|NO_QUERY|scotthamilton|is upset that he can t update his facebook by texting it    and might cry as a result  school today also  blah |\n",
      "+---+----------+----------+--------+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75fd5cc",
   "metadata": {},
   "source": [
    "## Sentiment Analysis without Tokenization, Lemmatization and Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e939c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_sentiment_1 = df.select('date', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd00a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vader SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function for UDF\n",
    "def analyze_sentiment(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "# Save UDF\n",
    "sentiment_udf = udf(analyze_sentiment, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a26f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Vader analysis and add the results to a new column\n",
    "df_for_sentiment_1 = df_for_sentiment_1.withColumn(\"sentiment_score\", sentiment_udf(df_for_sentiment_1[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54cbd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"date\" column to 'yyyy-MM-dd' format\n",
    "df_for_sentiment_1 = df_for_sentiment_1.withColumn(\"date\", F.to_date(df_for_sentiment_1[\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd48202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you're using the correct column name in the aggregation\n",
    "daily_sentiment_1 = df_for_sentiment_1.groupBy(\"date\").agg(avg(\"sentiment_score\").alias(\"avg_sentiment_score\")).orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfbbd7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a full date array to include the entire date range\n",
    "min_date = daily_sentiment_1.selectExpr(\"min(date) as min_date\").first().min_date\n",
    "max_date = daily_sentiment_1.selectExpr(\"max(date) as max_date\").first().max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c44a97d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date array\n",
    "date_range = [min_date + timedelta(days=x) for x in range((max_date - min_date).days + 1)]\n",
    "date_range_df = spark.createDataFrame([(date,) for date in date_range], [\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54729282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the gap in date column\n",
    "daily_sentiment_1 = date_range_df.join(daily_sentiment_1, on=[\"date\"], how=\"left\").orderBy(\"date\").fillna(0, subset=[\"avg_sentiment_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "361fff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- avg_sentiment_score: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_sentiment_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9bcefe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==================================================>   (187 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      date|avg_sentiment_score|\n",
      "+----------+-------------------+\n",
      "|2009-04-07| 0.1638104692791461|\n",
      "|2009-04-08|                0.0|\n",
      "|2009-04-09|                0.0|\n",
      "|2009-04-10|                0.0|\n",
      "|2009-04-11|                0.0|\n",
      "|2009-04-12|                0.0|\n",
      "|2009-04-13|                0.0|\n",
      "|2009-04-14|                0.0|\n",
      "|2009-04-15|                0.0|\n",
      "|2009-04-16|                0.0|\n",
      "|2009-04-17|                0.0|\n",
      "|2009-04-18|0.18738602157202913|\n",
      "|2009-04-19|0.18894100089100915|\n",
      "|2009-04-20|0.17782408521710552|\n",
      "|2009-04-21|0.17327567762269075|\n",
      "|2009-04-22|                0.0|\n",
      "|2009-04-23|                0.0|\n",
      "|2009-04-24|                0.0|\n",
      "|2009-04-25|                0.0|\n",
      "|2009-04-26|                0.0|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "daily_sentiment_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8ce4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the window function to fill the DataFrame with past and future values\n",
    "window_spec = Window.orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c304b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with 0\n",
    "daily_sentiment_1 = daily_sentiment_1.fillna(0, subset=['avg_sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4599380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply Backward Fill interpolation\n",
    "daily_sentiment_1 = daily_sentiment_1.withColumn(\"backward_fill\", lag(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1988d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Apply Linear Interpolation\n",
    "daily_sentiment_1 = daily_sentiment_1.withColumn(\"linear_fill\", lag(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4803d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply Quadratic Interpolation\n",
    "daily_sentiment_1 = daily_sentiment_1.withColumn(\"quadratic_fill\", coalesce(\n",
    "    (lag(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec) + 2 * daily_sentiment_1[\"avg_sentiment_score\"] - lead(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec)),\n",
    "    daily_sentiment_1[\"avg_sentiment_score\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ccfee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Apply Mean of Nearest Neighbors interpolation\n",
    "daily_sentiment_1 = daily_sentiment_1.withColumn(\"knn_mean\", coalesce(\n",
    "    (daily_sentiment_1[\"avg_sentiment_score\"] + (lag(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec) + lead(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec)) / 2),\n",
    "    daily_sentiment_1[\"avg_sentiment_score\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c713b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Apply Mean of Seasonal Counterparts interpolation\n",
    "daily_sentiment_1 = daily_sentiment_1.withColumn(\"seasonal_mean\", coalesce(\n",
    "    (daily_sentiment_1[\"avg_sentiment_score\"] + (lag(daily_sentiment_1[\"avg_sentiment_score\"], 7).over(window_spec) + lead(daily_sentiment_1[\"avg_sentiment_score\"], -7).over(window_spec)) / 2),\n",
    "    daily_sentiment_1[\"avg_sentiment_score\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de7537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 16:10:30,517 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2023-11-10 16:10:30,545 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2023-11-10 16:10:30,545 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 13:=========>        (1 + 1) / 2][Stage 14:>                 (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Collect the results for visualization\n",
    "interpolated_data = daily_sentiment_1.select(\"date\", \"avg_sentiment_score\", \"backward_fill\", \"linear_fill\", \"quadratic_fill\", \"knn_mean\", \"seasonal_mean\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585faf7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract data for plotting\n",
    "dates = [row.date for row in interpolated_data]\n",
    "original_scores = [row.avg_sentiment_score for row in interpolated_data]\n",
    "interpolation_methods = [\"backward_fill\", \"linear_fill\", \"quadratic_fill\", \"knn_mean\", \"seasonal_mean\"]\n",
    "\n",
    "# Create a loop to plot each interpolation method\n",
    "for method in interpolation_methods:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dates, original_scores, label='Original')\n",
    "    plt.plot(dates, [row[method] for row in interpolated_data], label=method.replace(\"_\", \" \").title())  # Use the method name as the label\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.title(f'Original vs. {method.replace(\"_\", \" \").title()}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6831461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column names\n",
    "columns = daily_sentiment_1.columns\n",
    "\n",
    "# Find and print the count of null values in each column\n",
    "for column in columns:\n",
    "    null_count = daily_sentiment_1.filter(daily_sentiment_1[column].isNull()).count()\n",
    "    print(f\"Column '{column}' contains {null_count} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7d251",
   "metadata": {},
   "source": [
    "### Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db85fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window specification to order the data by date\n",
    "window_spec = Window.orderBy(\"date\")\n",
    "\n",
    "# Calculate the next non-null value using the 'last' function\n",
    "interpolated_df = daily_sentiment_1.withColumn(\"next_value\", F.last(\"avg_sentiment_score\", True).over(window_spec))\n",
    "\n",
    "# Calculate the previous non-null value using the 'first' function\n",
    "interpolated_df = interpolated_df.withColumn(\"prev_value\", F.first(\"avg_sentiment_score\", True).over(window_spec))\n",
    "\n",
    "# Calculate the linearly interpolated value\n",
    "interpolated_df = interpolated_df.withColumn(\n",
    "    \"interpolated_value\",\n",
    "    F.when(F.col(\"avg_sentiment_score\") == 0, (F.col(\"next_value\") + F.col(\"prev_value\")) / 2).otherwise(F.col(\"avg_sentiment_score\"))\n",
    ")\n",
    "\n",
    "# Drop the 'next_value' and 'prev_value' columns if not needed\n",
    "interpolated_df = interpolated_df.drop(\"next_value\", \"prev_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330457b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0763c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_df = interpolated_df.select('date', 'interpolated_value')\n",
    "interpolated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aaa5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column names\n",
    "columns = interpolated_df.columns\n",
    "\n",
    "# Find and print the count of null values in each column\n",
    "for column in columns:\n",
    "    null_count = interpolated_df.filter(interpolated_df[column].isNull()).count()\n",
    "    print(f\"Column '{column}' contains {null_count} null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de25838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DASH\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Configure DASH\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(\n",
    "        id='line-chart',\n",
    "        figure=px.line(interpolated_df, x='date', y='interpolated_value', title='Daily Average Sentiment Score')\n",
    "    )\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True,  port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c8418",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF and KPSS test\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from pyspark.sql.functions import col\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af055b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#windowSpec = Window.orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52251d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DF to NumPy array\n",
    "values = interpolated_df.select(\"interpolated_value\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF(Augmented Dickey Fuller) Test\n",
    "result_adf = adfuller(values, autolag='AIC')\n",
    "print(f'ADF Statistic: {result_adf[0]}')\n",
    "print(f'p-value: {result_adf[1]}')\n",
    "for key, value in result_adf[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f' {key}, {value}')\n",
    "\n",
    "# KPSS(Kwiatkowski-Phillips-Schmidt-Shin) Test\n",
    "result_kpss = kpss(values, regression='c')\n",
    "print('\\nKPSS Statistic: %f' % result_kpss[0])\n",
    "print('p-value: %f' % result_kpss[1])\n",
    "for key, value in result_kpss[3].items():\n",
    "    print('Critial Values:')\n",
    "    print(f' {key}, {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64fe78",
   "metadata": {},
   "source": [
    "The ADF test is commonly used to check if a time series has a unit root, indicating non-stationarity. If the p-value in the ADF test is below 0.05, you reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599aaf6b",
   "metadata": {},
   "source": [
    "The KPSS test is commonly used to check if a time series has a unit root, indicating non-stationarity. If the p-value in the ADF test is below 0.05, you conclude that the time series is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Take the DF from pyspark\n",
    "plt.plot(values, color='k')\n",
    "plt.title('Random White Noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84367b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detrend the DF\n",
    "detrended = np.array(values) - np.polyfit(range(len(values)), values, 1)[0] * np.arange(len(values))\n",
    "\n",
    "# Visualize the detrended DF\n",
    "plt.plot(detrended)\n",
    "plt.title('Sentiment Score detrended by subtracting the least squares fit', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc4042",
   "metadata": {},
   "source": [
    "# Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "interpolated_df_pd= interpolated_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f333e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for the Algorithm\n",
    "# ==============================================================================\n",
    "interpolated_df_pd['date'] = pd.to_datetime(interpolated_df_pd['date'], format='%Y/%m/%d')\n",
    "interpolated_df_pd = interpolated_df_pd.set_index('date')\n",
    "interpolated_df_pd = interpolated_df_pd.asfreq('D')\n",
    "interpolated_df_pd = interpolated_df_pd.sort_index()\n",
    "interpolated_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows with missing values: {interpolated_df_pd.isnull().any(axis=1).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that a temporary index is complete\n",
    "# ==============================================================================\n",
    "(interpolated_df_pd.index == pd.date_range(start=interpolated_df_pd.index.min(),\n",
    "end=interpolated_df_pd.index.max(),\n",
    "freq=interpolated_df_pd.index.freq)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259184fa",
   "metadata": {},
   "source": [
    "## AutoRegressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d07599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train-test\n",
    "# ==============================================================================\n",
    "\n",
    "# Define the number of steps for test data\n",
    "steps = 20\n",
    "\n",
    "# Split the data into training and test sets\n",
    "data_train = interpolated_df_pd[:-steps]\n",
    "data_test = interpolated_df_pd[-steps:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87723843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about the split\n",
    "print(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()} (n={len(data_train)})\")\n",
    "print(f\"Test dates : {data_test.index.min()} --- {data_test.index.max()} (n={len(data_test)}\")\n",
    "\n",
    "# Create a plot to visualize the train and test data\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "data_train['interpolated_value'].plot(ax=ax, label='train')\n",
    "data_test['interpolated_value'].plot(ax=ax, label='test')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dde9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define your lag variables\n",
    "# ==============================================================================\n",
    "data_train['lag1'] = data_train['interpolated_value'].shift(1)\n",
    "data_train['lag2'] = data_train['interpolated_value'].shift(2)\n",
    "\n",
    "# Drop missing values\n",
    "data_train = data_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77843d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train AR model\n",
    "# ==============================================================================\n",
    "X = data_train[['lag1', 'lag2']]\n",
    "X = sm.add_constant(X)  # Add a constant for the intercept\n",
    "y = data_train['interpolated_value']\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73616dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lag values\n",
    "lag1 = data_train['lag1'].iloc[-1]  \n",
    "lag2 = data_train['lag2'].iloc[-1]  \n",
    "\n",
    "\n",
    "# Predict 'steps' time periods into the future\n",
    "forecasted_values = []\n",
    "\n",
    "for _ in range(steps):\n",
    "    # Calculate the prediction\n",
    "    y_pred = model.predict([1, lag1, lag2])\n",
    "    forecasted_values.append(y_pred[0])  # Access the first element directly\n",
    "\n",
    "    # Update lag values for the next prediction\n",
    "    lag2 = lag1\n",
    "    lag1 = y_pred[0]  # Access the first element directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the predictions with date index\n",
    "prediction_dates = data_test.index[-steps:]\n",
    "predictions = pd.DataFrame({'Predicted': forecasted_values}, index=prediction_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 rows of the predictions DataFrame\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0bbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bcb072",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a90db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# ==============================================================================\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "data_train['interpolated_value'].plot(ax=ax, label='train')\n",
    "data_test['interpolated_value'].plot(ax=ax, label='test')\n",
    "predictions.plot(ax=ax, label='predictions')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate the test error (MSE)\n",
    "error_mse = mean_squared_error(\n",
    "    y_true=data_test['interpolated_value'],\n",
    "    y_pred=predictions['Predicted']\n",
    ")\n",
    "\n",
    "# Print the test error\n",
    "print(f\"Test error (MSE): {error_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21efece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [1, 500],\n",
    "    'max_depth': [1, 2, 3, 4 ,5]\n",
    "}\n",
    "\n",
    "# Create the RandomForestRegressor\n",
    "regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Create Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Initialize variables to store the best hyperparameters and MSE\n",
    "best_params = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "# Perform LOOCV to search for the best hyperparameters\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Create GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters and the best model\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test data\n",
    "    forecasted_values = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mse = mean_squared_error(y_test, forecasted_values)\n",
    "\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"n_estimators:\", best_params['n_estimators'])\n",
    "print(\"max_depth:\", best_params['max_depth'])\n",
    "print(f\"Best MSE: {best_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the hyperparameters\n",
    "n_estimators = 500\n",
    "max_depth = 3\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# Create and train a RandomForestRegressor with the best hyperparameters\n",
    "regressor = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators, random_state=42)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "forecasted_values = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b15534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecasted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a4094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index for the forecasted values\n",
    "forecasted_index = data_test.index[:len(forecasted_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecasted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9c5c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a0a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tahmin değerleri ve indeksini bir DataFrame'e dönüştürme\n",
    "forecasted_data = pd.DataFrame({'Predicted': forecasted_values}, index=forecasted_index)\n",
    "\n",
    "# Sonuçları görüntüleme\n",
    "forecasted_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2a805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecasted_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "data_train['interpolated_value'].plot(ax=ax, label='train')\n",
    "data_test['interpolated_value'].plot(ax=ax, label='test')\n",
    "forecasted_data['Predicted'].plot(ax=ax, label='predicted')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, forecasted_values)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9814d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Fit a SARIMA model\n",
    "order = (2, 0, 0)  # Non-seasonal order\n",
    "seasonal_order = (1, 0, 1, 7)  # Seasonal order (p, d, q, S)\n",
    "\n",
    "model_sarima = SARIMAX(data_train['interpolated_value'], order=order, seasonal_order=seasonal_order)\n",
    "results_sarima = model_sarima.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "forecasted_values_sarima = results_sarima.predict(start=len(data_train), end=len(data_train) + len(data_test) - 1, typ='levels')\n",
    "\n",
    "# Create an index for the forecasted values\n",
    "forecasted_index_sarima = data_test.index[:len(forecasted_values_sarima)]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "data_train['interpolated_value'].plot(ax=ax, label='train')\n",
    "data_test['interpolated_value'].plot(ax=ax, label='test')\n",
    "forecasted_values_sarima.plot(ax=ax, label='SARIMA predictions', color='green')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the test error (MSE)\n",
    "error_mse_sarima = mean_squared_error(\n",
    "    y_true=data_test['interpolated_value'],\n",
    "    y_pred=forecasted_values_sarima\n",
    ")\n",
    "\n",
    "# Print the test error for SARIMA\n",
    "print(f\"SARIMA Test error (MSE): {error_mse_sarima}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Create lag variables\n",
    "data_train['lag1'] = data_train['interpolated_value'].shift(1)\n",
    "data_train['lag2'] = data_train['interpolated_value'].shift(2)\n",
    "# Plot ACF and PACF\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plot_acf(data_train['interpolated_value'], lags=25, ax=ax[0])\n",
    "plot_pacf(data_train['interpolated_value'], lags=25, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb7aa70",
   "metadata": {},
   "source": [
    "# Lemmatization, Tokenization, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb97431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization using NLTK\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "lemmatize_udf = udf(lemmatize_text, StringType())\n",
    "df = df.withColumn(\"text\", lemmatize_udf(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abfb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"filtered_words\")\n",
    "df = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0794fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StopWordsRemover on the \"filtered_words\" column in your example DataFrame\n",
    "remover = StopWordsRemover(inputCol=\"filtered_words\", outputCol=\"filtered_words_without_stopwords\")\n",
    "df = remover.transform(df)\n",
    "\n",
    "# You can update the column name as per your needs\n",
    "df = df.withColumnRenamed(\"filtered_words_without_stopwords\", \"filtered_words_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef86ef2",
   "metadata": {},
   "source": [
    "# Preparing the Dataset for the Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just pick the necessary columns\n",
    "df = df.select('0', 'ids', 'date', 'flag', 'user', 'filtered_words_final')\n",
    "\n",
    "# Rename the \"0\" column to \"index\"\n",
    "df = df.withColumnRenamed(\"0\", \"tweet_index\")\n",
    "\n",
    "# Show the result\n",
    "#df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of values in the dataframe\n",
    "#total_count = df.count()\n",
    "\n",
    "# Show the total count\n",
    "#print(\"Total count of values in the dataframe\", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79986ea3",
   "metadata": {},
   "source": [
    "# MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"password\",\n",
    "    database=\"ProjectTweets\",\n",
    "    charset='utf8mb4',\n",
    "    cursorclass=pymysql.cursors.DictCursor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a cursor\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# # Create a table\n",
    "# create_table_sql = \"\"\"\n",
    "# CREATE TABLE Tweets (\n",
    "#     tweet_index INT AUTO_INCREMENT PRIMARY KEY,\n",
    "#     ids BIGINT,\n",
    "#     date DATE,\n",
    "#     flag VARCHAR(55),\n",
    "#     user VARCHAR(255),\n",
    "#     filtered_words_final TEXT\n",
    "# );\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table\n",
    "cursor.execute(create_table_sql)\n",
    "\n",
    "# Save changes\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aeee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the column named 'filtered_words_final' into a comma-separated column of text.\n",
    "df = df.withColumn('concatenated_words', concat_ws(\",\", df['filtered_words_final']))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('tweet_index', 'ids', 'date', 'flag', 'user', 'concatenated_words')\n",
    "df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebaf4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_url = \"jdbc:mysql://localhost:3306/ProjectTweets\"\n",
    "mysql_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60333aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.jdbc(url=mysql_url, table=\"Tweets\", mode=\"overwrite\", properties=mysql_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Execute the ALTER TABLE query\n",
    "# alter_table_sql = \"ALTER TABLE Tweets ADD COLUMN YCSB_KEY VARCHAR(255);\"\n",
    "# cursor.execute(alter_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc894c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c638b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_mysql = spark.read.jdbc(url=mysql_url, table=\"Tweets\", properties=mysql_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb2da0",
   "metadata": {},
   "source": [
    "### Showing results from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the database after insertin the dataframe\n",
    "df_from_mysql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1206f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a cursor\n",
    "# cursor = connection.cursor()\n",
    "\n",
    "# # Create a table\n",
    "# create_table_sql = \"\"\"\n",
    "# CREATE TABLE YCSB_TEST (\n",
    "#     tweet_index INT AUTO_INCREMENT PRIMARY KEY,\n",
    "#     ids BIGINT,\n",
    "#     date DATE,\n",
    "#     flag VARCHAR(55),\n",
    "#     user VARCHAR(255),\n",
    "#     filtered_words_final TEXT,\n",
    "#     YCSB_KEY VARCHAR(255)\n",
    "# );\n",
    "# \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a table\n",
    "cursor.execute(create_table_sql)\n",
    "\n",
    "#Save changes\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6340c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = \"/home/hduser/ycsb-0.17.0/bin/ycsb.sh load jdbc -P /home/hduser/ycsb-0.17.0/jdbc-binding/conf/db.properties -P /home/hduser/ycsb-0.17.0/workloads/workloada -p db.connection_properties=\\\"user=root&password=password&useSSL=false\\\" -p jdbc.url=jdbc:mysql://localhost:3306/ProjectTweets -p table=YCSB_TEST\"\n",
    "\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "if process.returncode == 0:\n",
    "    print(\"YCSB operation completed successfully.\")\n",
    "    print(\"Output:\")\n",
    "    print(stdout.decode('utf-8'))\n",
    "else:\n",
    "    print(\"YCSB operation failed. Error message:\")\n",
    "    print(stderr.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c9c746",
   "metadata": {},
   "source": [
    "### Due to unidentified Issue YCSB did not work. I decided to use cProfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb24e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek bir sorgu\n",
    "query = \"SELECT * FROM Tweets WHERE concatenated_words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d222c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "def perform_query():\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    cursor.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cProfile.run(\"perform_query()\", sort=\"cumulative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f15c6d",
   "metadata": {},
   "source": [
    "    Total calls: 1,131,181\n",
    "    Total time: 3.526 seconds\n",
    "\n",
    "Top time-consuming functions:\n",
    "\n",
    "    {built-in method builtins.exec}: 3.526 seconds\n",
    "    <string>:1(<module>): 3.526 seconds\n",
    "    3302925674.py:3(perform_query): 3.524 seconds\n",
    "    cursors.py:133(execute): 3.524 seconds\n",
    "    cursors.py:319(_query): 3.524 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee114bb",
   "metadata": {},
   "source": [
    "# Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"temp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b560d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ProjectTweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96c9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ProjectTweets.Tweets (\n",
    "    tweet_index INT,\n",
    "    ids BIGINT,\n",
    "    date DATE,\n",
    "    flag STRING,\n",
    "    user STRING,\n",
    "    concatenated_words STRING\n",
    ")\n",
    "STORED AS PARQUET\n",
    "\"\"\"\n",
    "spark.sql(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_insert_data_sql = \"\"\"\n",
    "INSERT INTO ProjectTweets SELECT * FROM temp_table\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a95903",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(hive_insert_data_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7974c",
   "metadata": {},
   "source": [
    "### Showing results from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d910743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "result = spark.sql(\"SELECT * FROM ProjectTweets\")\n",
    "\n",
    "# Show Result\n",
    "#result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = \"/home/hduser/ycsb-0.17.0/bin/ycsb.sh load jdbc -P /home/hduser/ycsb-0.17.0/jdbc-binding/conf/db.properties -P /home/hduser/ycsb-0.17.0/workloads/workloada -p db.connection_properties=\\\"user=root&password=password\\\" -p jdbc.url=jdbc:hive2://hive_server:10000/ProjectTweets\"\n",
    "\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "if process.returncode == 0:\n",
    "    print(\"YCSB operation completed successfully.\")\n",
    "    print(\"Output:\")\n",
    "    print(stdout.decode('utf-8'))\n",
    "else:\n",
    "    print(\"YCSB operation failed. Error message:\")\n",
    "    print(stderr.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece01872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_hive_script.py\n",
    "def hive_query():\n",
    "    query\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cProfile.run(\"hive_query()\", sort=\"cumulative\")\n",
    "    \n",
    "    # Show the results with pstat\n",
    "    p = pstats.Stats()\n",
    "    p.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c848c",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS AFTER TOKENIZATION, LEMMATIZATION AND STOPWORDS REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_sentiment_2 = df.select('date', 'concatenated_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vader SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function for the UDF\n",
    "def analyze_sentiment(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "# Save UDF\n",
    "sentiment_udf = udf(analyze_sentiment, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Vader analysis and add the results to a new column\n",
    "df_for_sentiment_2 = df_for_sentiment_2.withColumn(\"sentiment_score\", sentiment_udf(df_for_sentiment_2[\"concatenated_words\"]))\n",
    "\n",
    "df_for_sentiment_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"date\" column to 'yyyy-MM-dd' format\n",
    "df_for_sentiment_2 = df_for_sentiment_2.withColumn(\"date\", F.to_date(df_for_sentiment_2[\"date\"]))\n",
    "\n",
    "# Create a data frame for date and average sentiment scores\n",
    "daily_sentiment_2 = df_for_sentiment_2.groupBy(\"date\").agg(avg(\"sentiment_score\").alias(\"avg_sentiment_score\")).orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31902890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full date array to include the entire date range\n",
    "min_date = daily_sentiment_2.selectExpr(\"min(date) as min_date\").first().min_date\n",
    "max_date = daily_sentiment_2.selectExpr(\"max(date) as max_date\").first().max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the date array\n",
    "date_range = [min_date + timedelta(days=x) for x in range((max_date - min_date).days + 1)]\n",
    "date_range_df = spark.createDataFrame([(date,) for date in date_range], [\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing dates\n",
    "daily_sentiment_2 = date_range_df.join(daily_sentiment_2, on=[\"date\"], how=\"left\").orderBy(\"date\").fillna(0, subset=[\"avg_sentiment_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f15bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Dash application\n",
    "app = dash.Dash(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c418ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the layout of the application\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(\n",
    "        id='sentiment-line-chart',\n",
    "        figure=px.line(daily_sentiment_2, x='date', y='avg_sentiment_score', title='Daily Average Sentiment Score')\n",
    "    )\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, port=8880)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b4213",
   "metadata": {},
   "source": [
    "## This is why I chose the dataframe without lemmatization, tokenization, and stop word removal.\n",
    "- Because there is not enough sentiment score it is almost 0.\n",
    "- In this case removing special symbols and making the text letters smaller helped to keep the meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba1826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff396c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1cbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aec0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fea9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea7ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919784e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f36d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e75eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
